%        File: task-oriented_parser_evaluation.tex
%     Created: 周五 三月 02 04:00 下午 2012 W EST
% Last Change: 周五 三月 02 04:00 下午 2012 W EST
%
\documentclass[a4paper]{article}
%\usepackage{natbib}
%\usepackage{biblatex}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage[style=authoryear,natbib=true]{biblatex}
\bibliography{D:/Research/bib/Parsing}
\usepackage{graphicx}
%\usepackage{gb4e}
\usepackage{lingmacros}
\title{Report on our solution to Dialog Act prediction project}
\author{Wei Qiu \and Natalia Korchagina \and Bruno Andriamiarina}
\date{}
\begin{document}
\maketitle
\begin{abstract}
One of the most important tasks in dialog system design is classification consisting in assigning, out of a finite set, a specific category to each spoken utterance. These categories help guide the dialog manager in formulating a response to the speaker. Classification in general is a standard machine learning problem. A classification algorithm receives a finite number of labeled examples which it uses for training, and selects a hypothesis expected to make few errors on future examples. The following paper presents the results of a student project on corpus processing, where we had to train a classifier with a supervised method for simulating a Question Answer system. A classifier based on Conditional Random Fields (CRFs) was chosen by our group. \\
In the first section we will describe the process of the data collection, and will give some characteristics of the corpus we worked with. We will also present the results of our manual corpus annotation and will give some remarks on the annotation tags. In the second part we will talk about the importance of dialog structure detection and the methods of doing it. In the third section we will discuss the application of the sequential labeling methods to our task, explaining the choice of the supervised method and why Dialog Act prediction is a different kind of sequential problem than POS tagging or speech recognition. After presenting the conducted experiment we will finish by a conclusion.\\
\end{abstract}
\section{Introduction}
We have been working with a French dialog corpus for ALLEGRO chatterbox. ALLEGRO (Adaptive Language Learning technology for the Greater Region) is a European project aiming to develop multimedia tools for foreign language learning, and to make these tools accessible for public.\\
The foreign language learning takes places in a virtual environment, Second Life. ALLEGRO disposes a dialog system, making the learning process more natural. \\
The virtual agent of this dialog system is represented by a cat, accompanying the learner on the ALLEGRO island in Second Life. In order to collect the necessary set of dialogs for the classifier’s training, our university group as well as some other invited people participated in dialog sessions with the ALLEGRO chatterbox. \\
\\
\\
\\
\textbf{Data collection}\\
For the dialog collection a “wizard of oz” approach was adopted, where the subject talks to what appears to be an automatic system, but the system’s responses are in fact generated by a human (the “wizard”) from another computer. “Wizard of oz” was represented by a web interface that supports asynchronous calls from the dialog architecture (Open Agent Architecture OAA).\\
Working in pairs, one of the participants performed the role of the player (Learner), answering to the question of the system, and the other one performed the role of the system (Mana), guiding the dialog and annotating every utterance with a dialog-move label. During these “wizard of oz” experiments 52 dialogs, 1849 utterances, 3386 sentences and 19591 words were collected. Here is some average statistics for our corpus:\\
\\
Average Lines Per Dialog = 35.0\\
Average Sentences Per Dialog = 65.0\\
Average Words Per Dialog = 376.0\\
Average Words Per Sentence = 5.0\\
Every utterance in the dialogs was tagged with the given annotation tags, representing the following dialog acts:\\
\\
REQUEST: ask(ready), ask(age), ask(gender), etc,\\
ANSWER: answer(ready), answer(not-ready)\\
INFORMATION: inform (table-objects), inform(cheese-where), etc.\\
CONFIRMATION: confirm(nettoie\_sol,preferred\_option), confirm(cheese-here,posible\_option), etc.\\
GREETING: greet\\
ACKNOWLEDGE: ack\\
OUT OF CONTEXT: other\\
DON'T KNOW: dont-know\\




\subsection{Inter-annotator agreement calculation}
We were asked to annotate manually 5\% of the whole corpus and compute the inter-annotator agreement.\\
These 5\% of the corpus correspond to approximately 2 dialogs, 76 lines to annotate.\\
Here, the corpus has been annotated by Natalia and then by Bruno.

\subsubsection{How to compute the Inter-annotator agreement ?}
The inter-annotator agreement is a coefficient computed to quantify how much 2 annotators agree on their annotation of the same corpus.\\
It uses a confusion matrix, in which each category represents each one of the dialog acts seen in the annotated.\\
And then, using that confusion matrix, it computes $\kappa$ which is a ratio interpreted as how much the 2 annotators agreed.

\subsubsection{Result and interpretation}
We obtain $\kappa \approx 0.75$.\\
This result means that the 2 annotators substantially agree.\\
This result might have been biased by the amount of data which is relatively low (only 76 lines to annotate), but globally, the 2 annotators agree.\\
This result also shows that, even if it's low ($1 - \kappa \approx 0.25$), the 2 annotators remain in desagreement on certain points of the annotation.

\subsection{Discussion about the annotations}
\begin{itemize}
    \item As we have seen it in the previous section, even though the 2 annotators substantially agree, there still remains some desagreement points about some acts.
According to the confusion matrix, disagreements mostly occur on the confirm acts that are mistook with acknowledgment acts. It may be useful to precise what is the difference between confirmation and acknowledgement in the scope of the annotation task.
\end{itemize}
Even if we have got a rather good percentage of inter-annotator agreement (~75\%), the 25\% that were incorrect (and, the two annotated and taken in account for inter-annotator agreement dialogs, represent only 5\% from the whole corpus, so it can happen that there are more uncertainties) tell us that there are some modifications to do about the annotation tags, because, if the annotators disagree on the attribution of some tags, is that there are not enough different kinds of tags capable to describe precisely the utterances, in such a way that it would not leave any doubt to annotators about the tags to put.\\

There are several well-known dialog annotation schemes, such as DAMSL (Dialog Act Markup in Several Layers) or ISO standard for dialog acts. They both are multidimensional, thus allowing to annotate not only the communicative function and the semantic content, but different dialog acts, because an utterance often contains more than one dialog act at the same time, and, annotating them with the only one kind of dialog act, we cannot reflect the whole meaning, for example:\\
$>$\texttt{mana:Bienvenue dans ma maison , je suis le chat , veux tu jouer avec moi ?} $\|\|$ \emph{\small ask(ready)}\\
In this utterance we put the annotation ask(gender), and by this we annotated only one dialog act, the last one, (veux tu jouer avec moi?), but there are two others left unnoticed by our dialog system – Bienvenue dans ma maison (for example, some kind of a greeting function could be attriuted), and Je suis le chat (inform function).\\

So, inspired by ISO dialog standards, we can propose the following annotation for the previous example:\\
$>$\texttt{mana:Bienvenue dans ma maison $\|$  je suis le chat $\|$veux tu jouer avec moi ?}$\|\|$\\
\emph{\small Social obligation management Greeting $\|$ inform $\|$ question(ready)}\\

Speaking about the lack of precision in the given tag set, we have to mention that there are quite a lot of utterances that got the tag Other, because there weren’t any other tag that would be convenient. Instead, some other tags could be added. For example, for the following utterance:\\
$>$\texttt{mana:Viens , je vais te montrer la cuisine . } \emph{\small $\|\|$ other (given)}\\
We replace by\\
$>$\texttt{mana:Viens $\|$ je vais te montrer la cuisine .} $\|\|$ \\
\emph{\small action-directive $\|$ action-inform}\\

Thus, we suppose that the introducing of tagging that is multidimensional and contains more varied types of functions would make the annotation, and the classifier after it would have been trained with such a corpus, more precise.\\


\section{Background and previous work}
The ability to model and automatically detect discourse structure is crucial for any NLP applications. It's even more important in spoken dialog system because the system needs to take action based on the structure of dialog history.
Dialog act is a specialized speech act which normally includes acts such as promising, ordering, greeting, warning, inviting and congratulatiing. Different dialog systems may also introduce domain specific acts. 

Research in dialog acts increases since 1999, after spoken dialog  systems became commercial reality. There are generally two ways to identify speech act, namely script-based method and learning-based method. In our task it's actually a classifying problem rather than an identifying problem. So we here will generally give a short review of learning based methods in this area.

\section{Apply sequential labeling methods to this task}
\subsection{Basic introduction to CRFs}
In this project, we used a classifier based on Conditional Random Fields (CRFs).\\
Conditional Random Fields was introduced by John Lafferty, Andrew McCallum and Fernando Pereira.\\
Conditional Random Fields is a framework for building probabilistic models to segment and label sequence data.\\
Conditional Random Fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong -independence assumptions made in those models.\\
Conditional Random Fields also avoid a fundamental limitation of Maximum Entropy Markov Models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states.\\
Whereas an ordinary classifier predicts a label for a single sample without regard to "neighboring" samples, a CRF can take context into account.\\
Conditional Random Fields framework has all the advantages of Maximum entropy Markov Models but also solves the label bias problem in a principled way.\\
The critical difference between CRFs and MEMMs is that a MEMM uses per-state exponential models for the conditional probabilities of next states given the current state, while a CRF has a single exponential model for the joint probability of the entire sequence of labels given the observation sequence. Therefore, the weights of different features at different states can be traded off against each other.\\
A CRF is a finite state model with unnormalized transition probabilities.
\subsection{Dialog Act prediction as sequential labeling problem}
Strictly speaking Dialog Act prediction is quite different to typical sequential labeling problem such as POS tagging or speech recognition. The reason is that in traditional sequential labeling problem, the observed states which are required to be tagged are all available or at least a bunch of them are available as a minimal unit. However, in dialog act prediction problem, the observed states are dynamic and growing. And during run time, the dialog act predictor cares more about current state. Though the previous states may contain errors, the system would not have any interests to retag them and correct the mistakes in the history. This kinds of reflects the two different natures of theses two tasks.

However, we can still use sequential labeling methods to treat this problem. This is based on these observations:
\begin{itemize}
    \item First of all, we can consider the each dialog as a processing unit. During running time, we can consider the incomplete dialog as a unit to be tagged. Based on our observation, this way works in practice. We consider it may be relative to CRF's global optimization way to decode. 
    \item Secondly, in running time, there are actually two different types of tasks:
        \begin{itemize}
            \item recognize the speech act of the user. This is identical to sequential tagging problem since there is a observable variable for current hidden state.
            \item generate the right speech act of the system. This is different from sequential tagging problem because for the current hidden state, there is no observable data about it. To make sequential labeling work with it, we can shift the data alignment, e.g, we can consider the current hidden state is emitted from the previous observable data as demonstrated in Table \ref{table:system_align} 
        \end{itemize}
\end{itemize}

\begin{table}
    \begin{tabular}{|l|l|}
        \hline
        mana    & other              \\ \hline
        mana    & ask(age)           \\ \hline
        learner & answer(age)        \\ \hline
        mana    & ask(gender)        \\ \hline
        learner & answer(gender)     \\ \hline
        mana    & ask(firstLang)     \\ \hline
        learner & answer(firstLang)  \\ \hline
        mana    & ask(bilingue)      \\ \hline
        learner & answer(bilingue)   \\ \hline
        mana    & ask(secondLang)    \\ \hline
        learner & ?                  \\ \hline
    \end{tabular}
    \caption{To tag the content generated by user}
    \label{table:user_align}
\end{table}

\begin{table}
    \begin{tabular}{|l|l|}
        \hline
        mana    & Begin \\ \hline
        mana    & other              \\ \hline
        learner & ask(age)           \\ \hline
        mana    & answer(age)        \\ \hline
        learner & ask(gender)        \\ \hline
        mana    & answer(gender)     \\ \hline
        learner & ask(firstLang)     \\ \hline
        mana    & answer(firstLang)  \\ \hline
        learner & ask(bilingue)      \\ \hline
        mana    & answer(bilingue)   \\ \hline
        learner & ask(secondLang)    \\ \hline
        End     & ?                  \\ \hline
    \end{tabular}
    \caption{To tag the system move}
    \label{table:system_align}
\end{table}

\subsection{Features Engineering}
To make fully use the history and the content, tried out some features extracted from the dialog:
\begin{itemize}
    \item The agent of the current speech
    \item Whether the current speech consists of multiple sentences. This feature is based on the observation that for simple question, e,.g., ask(age), both the question and the answer would be very simple. But for some complicated questions such as ask(preference, drink), both of the question and the answer can be longer.
    \item Whether the last sentence of current speech is a question or not. This feature is intuitive.
    \item We also use the bag of words as feature. This encodes the actual content of the speech. For bag words features, we filter out the stop words, and use a French tokenizer called \href{https://github.com/boudinfl/kea}{KEA} to do the tokenization.
    \item we also planned to extract the POS information as features. However it turns out that to be not that easy to find a satisfying POS tagger.
\end{itemize}

For the feature template for CRF, we tried out two different templates:
\begin{itemize}
    \item We use the current speech features mentioned above but not including the BoW features, and the same features from the previous state. For binary feature template we only use the exactly previous predicted tag. This means all of the features we used are only based on current state and previous state(Table \ref{table:feature}). The reason that we don't want to incorporate longer history is due to two reasons:
        \begin{itemize}
            \item The corpus is relatively small, we don't want to overfit it
            \item The most important observation is that the dialogs are generated from the same source and almost by the same way. So there is an almost fixed pattern underneath. We consider that using long history would overfit the automata underneath.
        \end{itemize}
    \item the second template is an extension of the first one, we add BoW features of current state.
\end{itemize}
\begin{table}
    \begin{center}
    \begin{tabular}{|l|}
    \hline
    \#Unigram      \\ 
    U00:\%x[0,0]  \\ 
    U01:\%x[0,1]  \\ 
    U02:\%x[0,2]  \\ 
    U04:\%x[-1,0] \\ 
    U05:\%x[-1,1] \\ 
    U06:\%x[-1,2] \\ 
    \#Bigram       \\
    B\\
    \hline
    \end{tabular}
    \caption{Feature tempate for CRF++}
    \label{table:feature}
    \end{center}
\end{table}
\section{Implementation \& Experiment}
Our whole solution is based on \href{http://crfpp.googlecode.com/svn/trunk/doc/index.html}{CRF++}, \href{http://scikit-learn.org/stable/}{scikit-learn}, \href{http://nltk.org/}{NLTK}. CRF++ is a implementation of CRF which provides a data format which is easy to follow. We use it to do both the training and decode. It's written in C++ but provides various bindings for other languages. Scikit-learn is a machine learning toolset for python. We only use it to build the vector space model. NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. We only use its French sentence splitter and its French stopping words corpus.

10 fold cross validation is used to evaluate our result. As shown in Table \ref{tab:nonbow} and Table \ref{tab:bow} the bow features improed our result by around 7 percentage which is significant.

We also see that in the annotation there is typo! ``possible'' and ``posible'' are considered as two different tags. We believe that this also contributes the error. Also we can observe that due to the unbalance in the training data, it's hard to predict tags which rarely appear such as answer(not-ready) and reqRep 
\begin{longtable}{|l|l|l|l|}
    \hline
    & Precision &  Recall  & F \\\hline
    answer(age) &  100.00\% & 100.00\% & 100.00 \\
    answer(bilingue) &    0.00\% &   0.00\% &   0.00 \\
    %answer(cheese-where,posible\_option) &  100.00\% & 100.00\% & 100.00 \\
    answer(cheese-where,preferred\_option) &   83.33\% & 100.00\% &  90.91 \\
    answer(drink,posible\_option) &   50.00\% & 100.00\% &  66.67 \\
    answer(drink,preferred\_option) &  100.00\% &  80.00\% &  88.89 \\
    answer(firstLang) &  100.00\% & 100.00\% & 100.00 \\
    answer(frenchContact) &  100.00\% & 100.00\% & 100.00 \\
    answer(gender) &  100.00\% & 100.00\% & 100.00 \\
    answer(health) &  100.00\% & 100.00\% & 100.00 \\
    answer(health,posible\_option) &  100.00\% &  50.00\% &  66.67 \\
    answer(health,preferred\_option) &   75.00\% & 100.00\% &  85.71 \\
    answer(nettoie\_sol,dont\_know) &  100.00\% &  50.00\% &  66.67 \\
    answer(nettoie\_sol,posible\_option) &  100.00\% &  33.33\% &  50.00 \\
    answer(nettoie\_sol,possible\_option) &    0.00\% &   0.00\% &   0.00 \\
    answer(nettoie\_sol,preferred\_option) &   50.00\% & 100.00\% &  66.67 \\
    answer(not-ready) &    0.00\% &   0.00\% &   0.00 \\
    answer(preferred\_meal,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
    answer(preferred\_meal,preferred\_option) &   83.33\% & 100.00\% &  90.91 \\
    answer(ready) &   83.33\% & 100.00\% &  90.91 \\
    answer(secondLang) &    0.00\% &   0.00\% &   0.00 \\
    answer(table-objects,dont\_know) &    0.00\% &   0.00\% &   0.00 \\
    answer(table-objects,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
    answer(table-objects,preferred\_option) &   66.67\% &  80.00\% &  72.73 \\
    ask(age) &  100.00\% & 100.00\% & 100.00 \\
    ask(bilingue) &  100.00\% & 100.00\% & 100.00 \\
    ask(cheese-where) &  100.00\% & 100.00\% & 100.00 \\
    ask(drink) &  100.00\% & 100.00\% & 100.00 \\
    ask(firstLang) &  100.00\% & 100.00\% & 100.00 \\
    ask(frenchContact) &  100.00\% & 100.00\% & 100.00 \\
    ask(gender) &  100.00\% & 100.00\% & 100.00 \\
    ask(health) &  100.00\% & 100.00\% & 100.00 \\
    ask(nettoie\_sol) &  100.00\% & 100.00\% & 100.00 \\
    ask(preferred\_meal) &  100.00\% & 100.00\% & 100.00 \\
    ask(ready) &  100.00\% & 100.00\% & 100.00 \\
    ask(table-objects) &  100.00\% & 100.00\% & 100.00 \\
    confirm(cheese-where,posible\_option) &  100.00\% & 100.00\% & 100.00 \\
    confirm(cheese-where,preferred\_option) &   83.33\% & 100.00\% &  90.91 \\
    confirm(drink,posible\_option) &  100.00\% & 100.00\% & 100.00 \\
    confirm(drink,preferred\_option) &  100.00\% & 100.00\% & 100.00 \\
    confirm(health,posible\_option) &  100.00\% &  50.00\% &  66.67 \\
    confirm(health,preferred\_option) &   75.00\% & 100.00\% &  85.71 \\
    confirm(nettoie\_sol,posible\_option) &  100.00\% &  50.00\% &  66.67 \\
    confirm(nettoie\_sol,preferred\_option) &   66.67\% &  50.00\% &  57.14 \\
    confirm(preferred\_meal,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
    confirm(preferred\_meal,preferred\_option) &   83.33\% & 100.00\% &  90.91 \\
    confirm(table-objects,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
    confirm(table-objects,preferred\_option) &   66.67\% & 100.00\% &  80.00 \\
    inform(cheese-where) &  100.00\% & 100.00\% & 100.00 \\
    inform(drink) &  100.00\% & 100.00\% & 100.00 \\
    inform(health) &  100.00\% & 100.00\% & 100.00 \\
    inform(nettoie\_sol) &   25.00\% & 100.00\% &  40.00 \\
    inform(preferred\_meal) &  100.00\% & 100.00\% & 100.00 \\
    inform(table-objects) &  100.00\% & 100.00\% & 100.00 \\
    other   &   85.71\% &  92.31\% &  88.89 \\
    quit    &  100.00\% &  75.00\% &  85.71 \\
    reqRep  &    0.00\% &   0.00\% &   0.00 \\\hline
    Overall &   89.20\% &  89.20\% &  89.20 \\\hline
    \caption{Using BoW features}
    \label{tab:bow}
\end{longtable}

    \begin{longtable}{|l|l|l|l|}
        \hline
                & Precision &  Recall  & F \\\hline
        answer(age) &   85.71\% & 100.00\% &  92.31 \\
        answer(bilingue) &    0.00\% &   0.00\% &   0.00 \\
        answer(cheese-where,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
        answer(cheese-where,preferred\_option) &   71.43\% & 100.00\% &  83.33 \\
        answer(drink,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
        answer(drink,preferred\_option) &   83.33\% & 100.00\% &  90.91 \\
        answer(firstLang) &  100.00\% & 100.00\% & 100.00 \\
        answer(frenchContact) &  100.00\% & 100.00\% & 100.00 \\
        answer(gender) &  100.00\% & 100.00\% & 100.00 \\
        answer(health) &  100.00\% & 100.00\% & 100.00 \\
        answer(health,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
        answer(health,preferred\_option) &   75.00\% & 100.00\% &  85.71 \\
        answer(nettoie\_sol,dont\_know) &    0.00\% &   0.00\% &   0.00 \\
        answer(nettoie\_sol,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
        answer(nettoie\_sol,preferred\_option) &   16.67\% &  50.00\% &  25.00 \\
        answer(not-ready) &    0.00\% &   0.00\% &   0.00 \\
        answer(preferred\_meal,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
        answer(preferred\_meal,preferred\_option) &   83.33\% & 100.00\% &  90.91 \\
        answer(ready) &  100.00\% & 100.00\% & 100.00 \\
        answer(secondLang) &    0.00\% &   0.00\% &   0.00 \\
        answer(table-objects,dont\_know) &    0.00\% &   0.00\% &   0.00 \\
        answer(table-objects,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
        answer(table-objects,preferred\_option) &   66.67\% &  80.00\% &  72.73 \\
        ask(age) &   85.71\% & 100.00\% &  92.31 \\
        ask(bilingue) &    0.00\% &   0.00\% &   0.00 \\
        ask(cheese-where) &   85.71\% & 100.00\% &  92.31 \\
        ask(drink) &  100.00\% & 100.00\% & 100.00 \\
        ask(firstLang) &  100.00\% & 100.00\% & 100.00 \\
        ask(frenchContact) &  100.00\% & 100.00\% & 100.00 \\
        ask(gender) &  100.00\% & 100.00\% & 100.00 \\
        ask(health) &  100.00\% &  85.71\% &  92.31 \\
        ask(nettoie\_sol) &  100.00\% & 100.00\% & 100.00 \\
        ask(preferred\_meal) &  100.00\% & 100.00\% & 100.00 \\
        ask(ready) &  100.00\% &  83.33\% &  90.91 \\
        ask(secondLang) &    0.00\% &   0.00\% &   0.00 \\
        ask(table-objects) &  100.00\% & 100.00\% & 100.00 \\
        confirm(cheese-where,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
        confirm(cheese-where,preferred\_option) &   71.43\% & 100.00\% &  83.33 \\
        confirm(drink,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
        confirm(drink,preferred\_option) &   50.00\% & 100.00\% &  66.67 \\
        confirm(health,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
        confirm(health,preferred\_option) &   75.00\% & 100.00\% &  85.71 \\
        confirm(nettoie\_sol,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
        confirm(nettoie\_sol,preferred\_option) &   33.33\% &  50.00\% &  40.00 \\
        confirm(preferred\_meal,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
        confirm(preferred\_meal,preferred\_option) &   83.33\% & 100.00\% &  90.91 \\
        confirm(table-objects,posible\_option) &    0.00\% &   0.00\% &   0.00 \\
        confirm(table-objects,preferred\_option) &   66.67\% & 100.00\% &  80.00 \\
        inform(cheese-where) &  100.00\% & 100.00\% & 100.00 \\
        inform(drink) &  100.00\% & 100.00\% & 100.00 \\
        inform(health) &  100.00\% & 100.00\% & 100.00 \\
        inform(nettoie\_sol) &   33.33\% & 100.00\% &  50.00 \\
        inform(preferred\_meal) &   85.71\% & 100.00\% &  92.31 \\
        inform(table-objects) &  100.00\% & 100.00\% & 100.00 \\
        other   &   85.71\% &  92.31\% &  88.89 \\
        quit    &   85.71\% &  75.00\% &  80.00 \\
        reqRep  &    0.00\% &   0.00\% &   0.00 \\\hline
        Overall &   82.63\% &  82.63\% &  82.63 \\\hline
    \caption{Result not using BoW features}
    \label{tab:nonbow}
    \end{longtable}

\section{Conclusion \& Future Work}
In this project, we've applied sequential labeling to Dialog Act predict problem. We've shown that though this task is not a sequential labeling problem, we can still apply classical sequential labeling methods to it through some simple transformation.

The result showed that even with very simple features, CRF can still achieve relatively high performance on this task.

However, we can still see that sequential labeling methods couldn't fully solve this task. The main reason is due to the different natures of these two problems. We can foresee that for running time usage, our method's performance would drop significantly. 

A lot of researchers considers this task as partial observable Markov process, we also believe this method may capture the real structure of the task better. Our method just provides another perspective for this problem.

For the future work, more analysis on the error types need to be done. A bench system also needs to be built for better evaluation of the system. Also we didn't make any use of the internal structure of the tags. A hierarchical structured classifier may improve the performance. 

%\bibliographystyle{plainnat}
%\printbibliography
\end{document}
